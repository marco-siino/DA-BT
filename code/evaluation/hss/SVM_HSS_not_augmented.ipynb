{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/DA-BT/blob/main/code/evaluation/hss/SVM_HSS_not_augmented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "## Investigating text data augmentation using back translation for author profiling\n",
        "- - - \n",
        "SVM ON HSS DS EXPERIMENTS NOTEBOOK \n",
        "- - -\n",
        "Support Vector Machine on Hate Speech Spreaders Dataset not augmented with backtranslation.\n",
        "Code by M. Siino. \n",
        "\n",
        "From the paper: \"Investigating text data augmentation using back translation for author profiling\" by M.Siino et al.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AQSunQ-ucjLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412eb48c-1d30-4e5a-ab00-51d2466be9bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/dataset.py\n",
            "Fetching https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/vectorizer.py\n",
            "Fetching https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/simulator.py\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from urllib import request\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Import class Dataset\n",
        "module_url = f\"https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/dataset.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "from dataset import Dataset\n",
        "\n",
        "# Import class Vectorizer\n",
        "module_url = f\"https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/vectorizer.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "from vectorizer import Vectorizer\n",
        "\n",
        "# Import class Simulator\n",
        "module_url = f\"https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/simulator.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "from simulator import Simulator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch the dataset zips from GitHub and build up a Keras DS."
      ],
      "metadata": {
        "id": "ponyShZZABtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters of Dataset object are name of the ds used and augmentation language used.\n",
        "ds = Dataset('hss','original')\n",
        "ds.build_ds(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvKprkqkzBIm",
        "outputId": "6b0b6c83-50aa-424b-e71e-0841bf93a7eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/marco-siino/DA-BT/raw/main/data/hss/hss-train-original.zip\n",
            "1346942/1346942 [==============================] - 0s 0us/step\n",
            "Downloading data from https://github.com/marco-siino/DA-BT/raw/main/data/hss/hss-test-original.zip\n",
            "709016/709016 [==============================] - 0s 0us/step\n",
            "Found 200 files belonging to 2 classes.\n",
            "Found 100 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize text accordingly to the train set."
      ],
      "metadata": {
        "id": "8Ga8vMTIANTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vct_layer_obj = Vectorizer(ds.train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x-QXirtAAFU",
        "outputId": "fd18a2c5-9814-41ed-e7d6-e009c5ff3587"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the longest sample is: 4311\n",
            "\n",
            "Vocabulary size is: 57561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the simulation."
      ],
      "metadata": {
        "id": "ztD88sknARIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulator = Simulator (\"svm\",5,20,ds.train_set,ds.test_set,vct_layer_obj.vectorize_layer)\n",
        "simulator.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGT_dzZb94eM",
        "outputId": "2bb9008f-dfb8-4aa7-fcc6-1885d5521d38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setup for deterministic model completed.\n",
            "SVM Accuracy Score on Training set ->  1.0\n",
            "SVM Accuracy Score on Test set ->  0.59\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}