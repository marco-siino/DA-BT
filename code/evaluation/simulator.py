{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdh+xsfBHYcNqixokYjKe5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/DA-BT/blob/main/code/evaluation/simulator.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBrOnfAAjzgA"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"simulator.py\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1WpBKY4_pCE4EvJlPgo1v4HKQyXm4uSDR\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from sklearn import svm\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "# Import class Vectorizer\n",
        "module_url = f\"https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/vectorizer.py\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "from vectorizer import Vectorizer\n",
        "\n",
        "\n",
        "class Simulator:\n",
        "\n",
        "  def __init__(self, model, nr_runs, nr_epochs, train_set, test_set, vectorize_layer):\n",
        "    self.model = model\n",
        "    self.nr_runs = nr_runs\n",
        "    self.nr_epochs = nr_epochs\n",
        "    self.train_set = train_set\n",
        "    self.test_set = test_set\n",
        "    self.vectorize_layer = vectorize_layer\n",
        "    self.setup()\n",
        "\n",
        "  # Prior data and model setups before running.\n",
        "  def setup(self):\n",
        "    # To store maximum accuracy for each run.\n",
        "    self.runs_accuracy = []\n",
        "    \n",
        "    # Dictionary size.\n",
        "    self.max_features=len(self.vectorize_layer.get_vocabulary()) + 1\n",
        "\n",
        "    # Now specific setup parameters setup for each model.\n",
        "\n",
        "    if self.model==\"cnn\":\n",
        "      self.setup_shallow()\n",
        "      print(\"\\nSetup for shallow model completed.\")\n",
        "    \n",
        "    if self.model == \"roberta\":\n",
        "      self.setup_transformer()\n",
        "\n",
        "    if self.model == \"svm\":\n",
        "      print(\"\\nSetup for deterministic model completed.\")\n",
        "\n",
        "  def setup_shallow(self):\n",
        "    # Word embedding dimensions.\n",
        "    self.embedding_dim = 100\n",
        "\n",
        "    #For reproducibility.\n",
        "    tf.random.set_seed(1)   \n",
        "\n",
        "  def setup_transformer(self):\n",
        "    # Convert train and test keras DS into DFs.\n",
        "    self.train_df = [] # will contain text and label\n",
        "    for element in self.train_set:\n",
        "      authorDocument=element[0]\n",
        "      label=int(element[1].numpy())\n",
        "      #print(authorDocument[0])\n",
        "      text = Vectorizer.clean_samples(authorDocument[0].numpy()).numpy().decode('UTF-8')\n",
        "      self.train_df.append({\n",
        "          'text':text,\n",
        "          'label':label\n",
        "      })\n",
        "    self.train_df = pd.DataFrame(self.train_df)\n",
        "\n",
        "    self.test_df = [] # will contain text and label\n",
        "    for element in self.test_ds:\n",
        "      authorDocument=element[0]\n",
        "      label=int(element[1].numpy())\n",
        "      #print(authorDocument[0])\n",
        "      text = Vectorizer.clean_samples(authorDocument[0].numpy()).numpy().decode('UTF-8')\n",
        "      self.test_df.append({\n",
        "          'text':text,\n",
        "          'label':label\n",
        "      })\n",
        "    self.test_df = pd.DataFrame(self.test_df)\n",
        "\n",
        "\n",
        "  def run(self):\n",
        "    if self.model == \"cnn\":\n",
        "      self.run_cnn()\n",
        "    if self.model == \"svm\":\n",
        "      self.run_svm()\n",
        "    if self.model == \"roberta\":\n",
        "      self.run_roberta()\n",
        "\n",
        "  def run_cnn(self):\n",
        "    for run in range(1,(self.nr_runs+1)):\n",
        "      epochs_accuracy = []\n",
        "      model = tf.keras.Sequential([\n",
        "                                      tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                                      self.vectorize_layer,\n",
        "                                      layers.Embedding(self.max_features + 1, self.embedding_dim),                     \n",
        "                                      layers.Dropout(0.8),\n",
        "\n",
        "                                      layers.Conv1D(256,16,activation='relu'),\n",
        "                                      layers.MaxPooling1D(),\n",
        "                                      layers.Dropout(0.6),\n",
        "\n",
        "                                      layers.Dense(512,activation='relu'),\n",
        "                            \n",
        "                                      layers.GlobalAveragePooling1D(),\n",
        "                                      layers.Dropout(0.2),\n",
        "                                      layers.Dense(1)                            \n",
        "      ])\n",
        "      model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer='RMSprop', metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) \n",
        "\n",
        "      for epoch in range (0,self.nr_epochs):\n",
        "          history = model.fit(\n",
        "            self.train_set,\n",
        "            validation_data = self.test_set,\n",
        "            epochs=1,\n",
        "            shuffle=False,\n",
        "            # Comment the following line to do not save and download the model.\n",
        "            #callbacks=[callbacks]\n",
        "            )\n",
        "          accuracy = history.history['val_binary_accuracy']\n",
        "          print(\"Run: \",run,\"/ Accuracy on test set at epoch \",epoch,\" is: \", accuracy[0],\"\\n\")\n",
        "          epochs_accuracy.append(accuracy[0])\n",
        "\n",
        "      print(\"Accuracies over epochs:\",epochs_accuracy,\"\\n\")\n",
        "      self.runs_accuracy.append(max(epochs_accuracy))\n",
        "\n",
        "    self.runs_accuracy.sort()\n",
        "    print(\"\\n\\n Over all runs maximum accuracies on test set are:\", self.runs_accuracy)\n",
        "    print(\"The median is:\", self.runs_accuracy[2],\"\\n\\n\\n\")\n",
        "\n",
        "    # Final Result on test set\n",
        "    if (self.runs_accuracy[2]-self.runs_accuracy[0])>(self.runs_accuracy[4]-self.runs_accuracy[2]):\n",
        "      max_range_from_median = self.runs_accuracy[2]-self.runs_accuracy[0]\n",
        "    else:\n",
        "      max_range_from_median = self.runs_accuracy[4]-self.runs_accuracy[2]\n",
        "    final_result = str(self.runs_accuracy[2])+\" +/- \"+ str(max_range_from_median)\n",
        "    print(\"CNN Accuracy Score on test set -> \",final_result)\n",
        "\n",
        "  def run_svm(self):\n",
        "    # # # - - - - - MODELS DEFINITION AND EVALUATION - - - - - # # #\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "    model.add(self.vectorize_layer)\n",
        "\n",
        "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
        "\n",
        "    training_labels=[]\n",
        "    training_samples=[]\n",
        "\n",
        "    max_features=len(self.vectorize_layer.get_vocabulary()) + 1\n",
        "\n",
        "    for element in self.train_set:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "      \n",
        "      #print(\"Sample considered is: \", authorDocument[0])\n",
        "      #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "      #print(\"And has label: \", label[0].numpy())\n",
        "      \n",
        "      text_vect_layer_model = tf.keras.Model(inputs=model.input,\n",
        "                                          outputs=model.layers[0].output)\n",
        "      text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "      training_labels.append(label[0].numpy())\n",
        "      current_sample=np.zeros(max_features)\n",
        "      for current_token in text_vect_out[0][:].numpy():\n",
        "        #print(current_token,end=' ')\n",
        "        #print(vectorize_layer.get_vocabulary()[current_token])\n",
        "        current_sample[current_token]+=1\n",
        "      training_samples.append(current_sample)\n",
        "      #break\n",
        "\n",
        "    training_labels=np.array(training_labels)\n",
        "    training_samples=np.array(training_samples)\n",
        "    #print(\"\\nLE LABELS DEI CAMPIONI DI TRAINING SONO:\")\n",
        "    #print(training_labels)\n",
        "    #print(\"\\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:\")\n",
        "    #print(training_samples)\n",
        "\n",
        "    test_labels=[]\n",
        "    test_samples=[]\n",
        "\n",
        "    for element in self.test_set:\n",
        "      authorDocument=element[0]\n",
        "      label=element[1]\n",
        "      \n",
        "      text_vect_layer_model = tf.keras.Model(inputs=model.input,\n",
        "                                          outputs=model.layers[0].output)\n",
        "      text_vect_out = text_vect_layer_model(authorDocument)\n",
        "\n",
        "      test_labels.append(label[0].numpy())\n",
        "      current_sample=np.zeros(max_features)\n",
        "      for current_token in text_vect_out[0][:].numpy():\n",
        "        current_sample[current_token]+=1\n",
        "      test_samples.append(current_sample)\n",
        "\n",
        "    test_labels=np.array(test_labels)\n",
        "    test_samples=np.array(test_samples)\n",
        "\n",
        "    SVM = svm.SVC(C=0.5, kernel='linear', gamma='auto')\n",
        "    SVM.fit(training_samples,training_labels)\n",
        "    # predict the labels on training set\n",
        "    #predictions_SVM = SVM.predict(training_samples)\n",
        "    # Use accuracy_score function to get the accuracy\n",
        "    result=SVM.score(training_samples,training_labels)\n",
        "    print(\"SVM Accuracy Score on Training set -> \",result)\n",
        "\n",
        "    # predict the labels on validation dataset\n",
        "    predictions_SVM = SVM.predict(test_samples)\n",
        "    # Use accuracy_score function to get the accuracy\n",
        "    result=SVM.score(test_samples,test_labels)\n",
        "    print(\"SVM Accuracy Score on Test set -> \",result)\n",
        "    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "  def run_roberta(self):\n",
        "\n",
        "    model_args = ClassificationArgs(num_train_epochs=1, \n",
        "                                        no_save=True, \n",
        "                                        no_cache=True, \n",
        "                                        overwrite_output_dir=True)\n",
        "\n",
        "    model = ClassificationModel(\"roberta\", \n",
        "                                    'roberta-base', \n",
        "                                    args = model_args, \n",
        "                                    num_labels=2, \n",
        "                                    use_cuda=cuda_available)\n",
        "\n",
        "    runs_accuracy = []\n",
        "\n",
        "    for run in range(1,(self.nr_runs+1)):\n",
        "      epochs_accuracy=[]\n",
        "      model = ClassificationModel(\"roberta\", \n",
        "                                      'roberta-base', \n",
        "                                      args = model_args, \n",
        "                                      num_labels=2, \n",
        "                                      use_cuda=cuda_available)\n",
        "      for epoch in range (0,self.nr_epochs):\n",
        "        print(\"\\nEPOCH NUMBER: \", epoch)\n",
        "        # train model\n",
        "        print(\"\\nNOW TRAIN THE MODEL.\")\n",
        "        model.train_model(self.train_df,show_running_loss=False)\n",
        "        print(\"\\nNOW EVALUATE THE TEST DF.\")\n",
        "        result, model_outputs, wrong_predictions = model.eval_model(self.test_df)\n",
        "        # Results on test set.\n",
        "        print(result)\n",
        "        correct_predictions = result['tp']+result['tn']\n",
        "        print(\"Correct predictions are: \",correct_predictions)\n",
        "        total_predictions = result['tp']+result['tn']+result['fp']+result['fn']\n",
        "        print(\"Total predictions are: \",total_predictions)\n",
        "        accuracy = correct_predictions/total_predictions\n",
        "        print(\"Accuracy on test set is:\",accuracy,\"\\n\\n\")\n",
        "        epochs_accuracy.append(accuracy)\n",
        "\n",
        "      print(epochs_accuracy)\n",
        "      runs_accuracy.append(max(epochs_accuracy))\n",
        "\n",
        "    runs_accuracy.sort()\n",
        "    print(\"\\n\\n Over all runs maximum accuracies are:\", runs_accuracy)\n",
        "    print(\"The median is:\",runs_accuracy[2])\n",
        "\n",
        "    if (runs_accuracy[2]-runs_accuracy[0])>(runs_accuracy[4]-runs_accuracy[2]):\n",
        "      max_range_from_median = runs_accuracy[2]-runs_accuracy[0]\n",
        "    else:\n",
        "      max_range_from_median = runs_accuracy[4]-runs_accuracy[2]\n",
        "    final_result = str(runs_accuracy[2])+\" +/- \"+ str(max_range_from_median)\n",
        "    print(\"RoBERTa Accuracy Score on Test set -> \",final_result)"
      ]
    }
  ]
}