# -*- coding: utf-8 -*-
"""simulator.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WpBKY4_pCE4EvJlPgo1v4HKQyXm4uSDR
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras import layers
from tensorflow.keras import losses
from sklearn import svm

class Simulator:

  def __init__(self, model, nr_runs, nr_epochs, train_set, test_set, vectorize_layer):
    self.model = model
    self.nr_runs = nr_runs
    self.nr_epochs = nr_epochs
    self.train_set = train_set
    self.test_set = test_set
    self.vectorize_layer = vectorize_layer
    self.setup()

  # Prior data and model setups before running.
  def setup(self):
    # To store maximum accuracy for each run.
    self.runs_accuracy = []
    
    # Dictionary size.
    self.max_features=len(self.vectorize_layer.get_vocabulary()) + 1

    # Now specific setup parameters setup for each model.

    if self.model=="cnn":
      self.setup_shallow()
      print("\nSetup for shallow model completed.")
    
    if self.model == "roberta":
      self.setup_transformer()

    if self.model == "svm":
      print("\nSetup for deterministic model completed.")

  def setup_shallow(self):
    # Word embedding dimensions.
    self.embedding_dim = 100

    #For reproducibility.
    tf.random.set_seed(1)    


  def run(self):
    if self.model == "cnn":
      self.run_cnn()
    if self.model == "svm":
      self.run_svm()

  def run_cnn(self):
    for run in range(1,(self.nr_runs+1)):
      epochs_accuracy = []
      model = tf.keras.Sequential([
                                      tf.keras.Input(shape=(1,), dtype=tf.string),
                                      self.vectorize_layer,
                                      layers.Embedding(self.max_features + 1, self.embedding_dim),                     
                                      layers.Dropout(0.8),

                                      layers.Conv1D(256,16,activation='relu'),
                                      layers.MaxPooling1D(),
                                      layers.Dropout(0.6),

                                      layers.Dense(512,activation='relu'),
                            
                                      layers.GlobalAveragePooling1D(),
                                      layers.Dropout(0.2),
                                      layers.Dense(1)                            
      ])
      model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer='RMSprop', metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) 

      for epoch in range (0,self.nr_epochs):
          history = model.fit(
            self.train_set,
            validation_data = self.test_set,
            epochs=1,
            shuffle=False,
            # Comment the following line to do not save and download the model.
            #callbacks=[callbacks]
            )
          accuracy = history.history['val_binary_accuracy']
          print("Run: ",run,"/ Accuracy on test set at epoch ",epoch," is: ", accuracy[0],"\n")
          epochs_accuracy.append(accuracy[0])

      print("Accuracies over epochs:",epochs_accuracy,"\n")
      self.runs_accuracy.append(max(epochs_accuracy))

    self.runs_accuracy.sort()
    print("\n\n Over all runs maximum accuracies on test set are:", self.runs_accuracy)
    print("The median is:", self.runs_accuracy[2],"\n\n\n")

    # Final Result on test set
    if (self.runs_accuracy[2]-self.runs_accuracy[0])>(self.runs_accuracy[4]-self.runs_accuracy[2]):
      max_range_from_median = self.runs_accuracy[2]-self.runs_accuracy[0]
    else:
      max_range_from_median = self.runs_accuracy[4]-self.runs_accuracy[2]
    final_result = str(self.runs_accuracy[2])+" +/- "+ str(max_range_from_median)
    print("CNN Accuracy Score on test set -> ",final_result)

  def run_svm(self):
    # # # - - - - - MODELS DEFINITION AND EVALUATION - - - - - # # #

    model = tf.keras.models.Sequential()
    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    model.add(self.vectorize_layer)

    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

    training_labels=[]
    training_samples=[]

    max_features=len(self.vectorize_layer.get_vocabulary()) + 1

    for element in self.train_set:
      authorDocument=element[0]
      label=element[1]
      
      #print("Sample considered is: ", authorDocument[0])
      #print("Preprocessed: ", str(custom_standardization(authorDocument[0].numpy())))
      #print("And has label: ", label[0].numpy())
      
      text_vect_layer_model = tf.keras.Model(inputs=model.input,
                                          outputs=model.layers[0].output)
      text_vect_out = text_vect_layer_model(authorDocument)

      training_labels.append(label[0].numpy())
      current_sample=np.zeros(max_features)
      for current_token in text_vect_out[0][:].numpy():
        #print(current_token,end=' ')
        #print(vectorize_layer.get_vocabulary()[current_token])
        current_sample[current_token]+=1
      training_samples.append(current_sample)
      #break

    training_labels=np.array(training_labels)
    training_samples=np.array(training_samples)
    #print("\nLE LABELS DEI CAMPIONI DI TRAINING SONO:")
    #print(training_labels)
    #print("\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:")
    #print(training_samples)

    test_labels=[]
    test_samples=[]

    for element in self.test_set:
      authorDocument=element[0]
      label=element[1]
      
      text_vect_layer_model = tf.keras.Model(inputs=model.input,
                                          outputs=model.layers[0].output)
      text_vect_out = text_vect_layer_model(authorDocument)

      test_labels.append(label[0].numpy())
      current_sample=np.zeros(max_features)
      for current_token in text_vect_out[0][:].numpy():
        current_sample[current_token]+=1
      test_samples.append(current_sample)

    test_labels=np.array(test_labels)
    test_samples=np.array(test_samples)

    SVM = svm.SVC(C=0.5, kernel='linear', gamma='auto')
    SVM.fit(training_samples,training_labels)
    # predict the labels on training set
    #predictions_SVM = SVM.predict(training_samples)
    # Use accuracy_score function to get the accuracy
    result=SVM.score(training_samples,training_labels)
    print("SVM Accuracy Score on Training set -> ",result)

    # predict the labels on validation dataset
    predictions_SVM = SVM.predict(test_samples)
    # Use accuracy_score function to get the accuracy
    result=SVM.score(test_samples,test_labels)
    print("SVM Accuracy Score on Test set -> ",result)
    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
