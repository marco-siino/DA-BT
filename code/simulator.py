# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QX97Wz8njlpFqrRaRd4s3aG85lM-Owdk
"""

# -*- coding: utf-8 -*-
"""simulator.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WpBKY4_pCE4EvJlPgo1v4HKQyXm4uSDR
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import torch
from tensorflow.keras import layers
from tensorflow.keras import losses
from sklearn import svm
from urllib import request
from simpletransformers.classification import ClassificationModel, ClassificationArgs

# Import class Vectorizer
module_url = f"https://raw.githubusercontent.com/marco-siino/DA-BT/main/code/vectorizer.py"
module_name = module_url.split('/')[-1]
print(f'Fetching {module_url}')
with request.urlopen(module_url) as f, open(module_name,'w') as outf:
  a = f.read()
  outf.write(a.decode('utf-8'))
from vectorizer import Vectorizer


class Simulator:

  def __init__(self, model, nr_runs, nr_epochs, train_set, test_set, vectorize_layer):
    self.model = model
    self.nr_runs = nr_runs
    self.nr_epochs = nr_epochs
    self.train_set = train_set
    self.test_set = test_set
    self.vectorize_layer = vectorize_layer
    self.setup()

  # Prior data and model setups before running.
  def setup(self):
    # To store maximum accuracy for each run.
    self.runs_accuracy = []
    
    # Dictionary size.
    self.max_features=len(self.vectorize_layer.get_vocabulary()) + 1

    # Now specific setup parameters setup for each model.

    if self.model=="cnn":
      self.setup_shallow()
      print("\nSetup for shallow model completed.")
    
    if self.model == "roberta":
      self.setup_transformer()

    if self.model == "svm":
      print("\nSetup for deterministic model completed.")

  def setup_shallow(self):
    # Word embedding dimensions.
    self.embedding_dim = 100

    #For reproducibility.
    tf.random.set_seed(1)   

  def setup_transformer(self):
    # Convert train and test keras DS into DFs.
    self.train_df = [] # will contain text and label
    for element in self.train_set:
      authorDocument=element[0]
      label=int(element[1].numpy())
      #print(authorDocument[0])
      text = Vectorizer.clean_samples(authorDocument[0].numpy()).numpy().decode('UTF-8')
      self.train_df.append({
          'text':text,
          'label':label
      })
    self.train_df = pd.DataFrame(self.train_df)

    self.test_df = [] # will contain text and label
    for element in self.test_set:
      authorDocument=element[0]
      label=int(element[1].numpy())
      #print(authorDocument[0])
      text = Vectorizer.clean_samples(authorDocument[0].numpy()).numpy().decode('UTF-8')
      self.test_df.append({
          'text':text,
          'label':label
      })
    self.test_df = pd.DataFrame(self.test_df)


  def run(self):
    if self.model == "cnn":
      self.run_cnn()
    if self.model == "svm":
      self.run_svm()
    if self.model == "roberta":
      self.run_roberta()

  def run_cnn(self):
    for run in range(1,(self.nr_runs+1)):
      epochs_accuracy = []
      model = tf.keras.Sequential([
                                      tf.keras.Input(shape=(1,), dtype=tf.string),
                                      self.vectorize_layer,
                                      layers.Embedding(self.max_features + 1, self.embedding_dim),                     
                                      layers.Dropout(0.8),

                                      layers.Conv1D(256,16,activation='relu'),
                                      layers.MaxPooling1D(),
                                      layers.Dropout(0.6),

                                      layers.Dense(512,activation='relu'),
                            
                                      layers.GlobalAveragePooling1D(),
                                      layers.Dropout(0.2),
                                      layers.Dense(1)                            
      ])
      model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer='RMSprop', metrics=tf.metrics.BinaryAccuracy(threshold=0.0)) 

      for epoch in range (0,self.nr_epochs):
          history = model.fit(
            self.train_set,
            validation_data = self.test_set,
            epochs=1,
            shuffle=False,
            # Comment the following line to do not save and download the model.
            #callbacks=[callbacks]
            )
          accuracy = history.history['val_binary_accuracy']
          print("Run: ",run,"/ Accuracy on test set at epoch ",epoch," is: ", accuracy[0],"\n")
          epochs_accuracy.append(accuracy[0])

      print("Accuracies over epochs:",epochs_accuracy,"\n")
      self.runs_accuracy.append(max(epochs_accuracy))

    self.runs_accuracy.sort()
    print("\n\n Over all runs maximum accuracies on test set are:", self.runs_accuracy)
    print("The median is:", self.runs_accuracy[2],"\n\n\n")

    # Final Result on test set
    if (self.runs_accuracy[2]-self.runs_accuracy[0])>(self.runs_accuracy[4]-self.runs_accuracy[2]):
      max_range_from_median = self.runs_accuracy[2]-self.runs_accuracy[0]
    else:
      max_range_from_median = self.runs_accuracy[4]-self.runs_accuracy[2]
    final_result = str(self.runs_accuracy[2])+" +/- "+ str(max_range_from_median)
    print("CNN Accuracy Score on test set -> ",final_result)

  def run_svm(self):
    # # # - - - - - MODELS DEFINITION AND EVALUATION - - - - - # # #

    model = tf.keras.models.Sequential()
    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
    model.add(self.vectorize_layer)

    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

    training_labels=[]
    training_samples=[]

    max_features=len(self.vectorize_layer.get_vocabulary()) + 1

    for element in self.train_set:
      authorDocument=element[0]
      label=element[1]
      
      #print("Sample considered is: ", authorDocument[0])
      #print("Preprocessed: ", str(custom_standardization(authorDocument[0].numpy())))
      #print("And has label: ", label[0].numpy())
      
      text_vect_layer_model = tf.keras.Model(inputs=model.input,
                                          outputs=model.layers[0].output)
      text_vect_out = text_vect_layer_model(authorDocument)

      training_labels.append(label[0].numpy())
      current_sample=np.zeros(max_features)
      for current_token in text_vect_out[0][:].numpy():
        #print(current_token,end=' ')
        #print(vectorize_layer.get_vocabulary()[current_token])
        current_sample[current_token]+=1
      training_samples.append(current_sample)
      #break

    training_labels=np.array(training_labels)
    training_samples=np.array(training_samples)
    #print("\nLE LABELS DEI CAMPIONI DI TRAINING SONO:")
    #print(training_labels)
    #print("\nI SAMPLE DI TRAINING DOPO LA TEXT VECTORIZATION SONO:")
    #print(training_samples)

    test_labels=[]
    test_samples=[]

    for element in self.test_set:
      authorDocument=element[0]
      label=element[1]
      
      text_vect_layer_model = tf.keras.Model(inputs=model.input,
                                          outputs=model.layers[0].output)
      text_vect_out = text_vect_layer_model(authorDocument)

      test_labels.append(label[0].numpy())
      current_sample=np.zeros(max_features)
      for current_token in text_vect_out[0][:].numpy():
        current_sample[current_token]+=1
      test_samples.append(current_sample)

    test_labels=np.array(test_labels)
    test_samples=np.array(test_samples)

    SVM = svm.SVC(C=0.5, kernel='linear', gamma='auto')
    SVM.fit(training_samples,training_labels)
    # predict the labels on training set
    #predictions_SVM = SVM.predict(training_samples)
    # Use accuracy_score function to get the accuracy
    result=SVM.score(training_samples,training_labels)
    print("SVM Accuracy Score on Training set -> ",result)

    # predict the labels on validation dataset
    predictions_SVM = SVM.predict(test_samples)
    # Use accuracy_score function to get the accuracy
    result=SVM.score(test_samples,test_labels)
    print("SVM Accuracy Score on Test set -> ",result)
    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #

  def run_roberta(self):
    cuda_available = torch.cuda.is_available()

    model_args = ClassificationArgs(num_train_epochs=1, 
                                        no_save=True, 
                                        no_cache=True, 
                                        overwrite_output_dir=True)

    model = ClassificationModel("roberta", 
                                    'roberta-base', 
                                    args = model_args, 
                                    num_labels=2, 
                                    use_cuda=cuda_available)

    runs_accuracy = []

    for run in range(1,(self.nr_runs+1)):
      epochs_accuracy=[]
      model = ClassificationModel("roberta", 
                                      'roberta-base', 
                                      args = model_args, 
                                      num_labels=2, 
                                      use_cuda=cuda_available)
      for epoch in range (0,self.nr_epochs):
        print("\nEPOCH NUMBER: ", epoch)
        # train model
        print("\nNOW TRAIN THE MODEL.")
        model.train_model(self.train_df,show_running_loss=False)
        print("\nNOW EVALUATE THE TEST DF.")
        result, model_outputs, wrong_predictions = model.eval_model(self.test_df)
        # Results on test set.
        print(result)
        correct_predictions = result['tp']+result['tn']
        print("Correct predictions are: ",correct_predictions)
        total_predictions = result['tp']+result['tn']+result['fp']+result['fn']
        print("Total predictions are: ",total_predictions)
        accuracy = correct_predictions/total_predictions
        print("Accuracy on test set is:",accuracy,"\n\n")
        epochs_accuracy.append(accuracy)

      print(epochs_accuracy)
      runs_accuracy.append(max(epochs_accuracy))

    runs_accuracy.sort()
    print("\n\n Over all runs maximum accuracies are:", runs_accuracy)
    print("The median is:",runs_accuracy[2])

    if (runs_accuracy[2]-runs_accuracy[0])>(runs_accuracy[4]-runs_accuracy[2]):
      max_range_from_median = runs_accuracy[2]-runs_accuracy[0]
    else:
      max_range_from_median = runs_accuracy[4]-runs_accuracy[2]
    final_result = str(runs_accuracy[2])+" +/- "+ str(max_range_from_median)
    print("RoBERTa Accuracy Score on Test set -> ",final_result)
